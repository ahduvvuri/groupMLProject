{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #progress bar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed for this run is: 6077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f977806d290>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creates a random seed from 1 - 10000 reinitialized at runtime\n",
    "manualSeed = random.randint(1, 10000)\n",
    "print(\"The random seed for this run is:\", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing pathname\n",
    "os.path.exists(\"melPlots/drops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory where data lives\n",
    "dataroot = \"melPlots/drops\"\n",
    "\n",
    "#number of subprocesses to use for loading data\n",
    "workers = 2\n",
    "\n",
    "#batch size for training\n",
    "batch_size = 40\n",
    "\n",
    "#Size of square melspec images. Images will be resized to this size.\n",
    "image_size = 218\n",
    "\n",
    "#number of color channels in training images. Color images have 3.\n",
    "nc = 3\n",
    "\n",
    "#size of the z vector, aka input of the generator\n",
    "nz = 100\n",
    "\n",
    "#size of feature maps in generator, set to same size as melspec images.\n",
    "ngf = 218\n",
    "\n",
    "#size of feature maps in discriminator, set to same size as melspec images\n",
    "ndf = 218\n",
    "\n",
    "#number of training epochs, will be low for faster runtime\n",
    "num_epochs = 5\n",
    "\n",
    "#learning rate for optimizers, set low for low step size\n",
    "lr = 0.0005\n",
    "\n",
    "#Beta1 hyperparameter for Adam optimizer\n",
    "beta1 = 0.5\n",
    "\n",
    "#number of GPUs available. Use 0 for CPU mode, but this cluster has 1.\n",
    "ngpu = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cschanz/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "#we use the image folder directory to load the data\n",
    "#directory needs at least one subfolder/subdirectory\n",
    "#elsee it will throw an exception.\n",
    "dataset = dset.ImageFolder(root = dataroot,\n",
    "                          transform = transforms.Compose([\n",
    "                              transforms.ToTensor()\n",
    "                          ]))\n",
    "\n",
    "#create the dataloader that loads the data\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle = True, num_workers = workers)\n",
    "\n",
    "#this decides what we run the algorithm on. Here, we will use the GPU.\n",
    "if (torch.cuda.is_available() and ngpu > 0):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#follow paper guidelines on weights initialization\n",
    "def weights_init(network):\n",
    "    \n",
    "    #pulls what kind of layer it is\n",
    "    classname = network.__class__.__name__\n",
    "    \n",
    "    #if it is a convolutional layer\n",
    "    if classname.find('Conv') != -1:\n",
    "        \n",
    "        #reinitialize weights to have mean 0, std 0.02\n",
    "        nn.init.normal_(network.weight.data, 0.0, 0.02)\n",
    "        \n",
    "    #else if it is a batch norm layer\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        \n",
    "        #reinitialize weights to have mean 1, std 0.02\n",
    "        nn.init.normal_(network.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(network.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the generator class\n",
    "#note: since each layer is structured in terms of our preset dimensions\n",
    "#each layer should scale and output correctly\n",
    "#with an end output of (nc) x 218 x 218\n",
    "\n",
    "#layers are structured as in the paper\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "        \n",
    "            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    #each step forward will parse through the sequential NN structure\n",
    "    #defined above\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 1744, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1744, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(1744, 872, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(872, 436, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(436, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(436, 218, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(218, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(218, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#create the generator defined previously\n",
    "network_G = Generator(ngpu).to(device)\n",
    "\n",
    "#and initalize weights using the functioned defined previously\n",
    "network_G.apply(weights_init)\n",
    "\n",
    "#print the model to see the structure\n",
    "print(network_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that generator is made, define the discriminator\n",
    "#note: since each layer is structured in terms of our preset dimensions\n",
    "#each layer should scale and output correctly\n",
    "\n",
    "#main difference is using LeakyReLU instead of ReLU activation\n",
    "#using regular conv instead of transpose conv\n",
    "#and using Sigmoid instead of Tanh for the final layer\n",
    "\n",
    "#layers are as structured in the paper\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 218, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(218, 436, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(436, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(436, 872, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(872, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(872, 1744, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(1744, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(1744, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#create the generator defined previously\n",
    "network_D = Discriminator(ngpu).to(device)\n",
    "\n",
    "#and initalize weights using the functioned defined previously\n",
    "network_D.apply(weights_init)\n",
    "\n",
    "#print the model to see the structure\n",
    "print(network_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Binary Cross Entropy loss function as our loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "#Generate batch of latent vectors which the generator will learn\n",
    "#to map to realistic looking melspecs\n",
    "fixed_noise = torch.randn(218, nz, 1, 1, device = device)\n",
    "\n",
    "#label real and fake melspecs with real being 1 and fake being 0\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "#use Adam optimizers for generator and discriminator\n",
    "optimizer_D = optim.Adam(network_D.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_G = optim.Adam(network_G.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the Training Loop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([40])) that is different to the input size (torch.Size([4000])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-92946f7c462c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#calculate loss on real batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0merrorD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#calculate gradients for discriminator via backprop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2753\u001b[0m         raise ValueError(\n\u001b[1;32m   2754\u001b[0m             \u001b[0;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m             \u001b[0;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2756\u001b[0m         )\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([40])) that is different to the input size (torch.Size([4000])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "#the main training loop\n",
    "\n",
    "#initialize empty lists to record progress\n",
    "melspec_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting the Training Loop...\")\n",
    "#for each epoch\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    \n",
    "    #for each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        #train discriminator on the real melspecs\n",
    "        network_D.zero_grad()\n",
    "        \n",
    "        #format batch of real melspecs\n",
    "        real_cpu = data[0].to(device)\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "        \n",
    "        #forward pass real melspec batch through discriminator\n",
    "        output = network_D(real_cpu).view(-1)\n",
    "        \n",
    "        #calculate loss on real batch\n",
    "        errorD_real = criterion(output, label)\n",
    "        \n",
    "        #calculate gradients for discriminator via backprop.\n",
    "        errorD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        #then train discriminator on fakes\n",
    "        rand_noise = torch.randn(batch_size, nz, 1, 1, device = device)\n",
    "        \n",
    "        #make fake melspecs with generator and update labels\n",
    "        fake = network_G(rand_noise)\n",
    "        label.fill(fake_label)\n",
    "        \n",
    "        #use discriminator to classify fakes\n",
    "        output = network_D(fake.detach()).view(-1)\n",
    "        \n",
    "        #calculate loss of discriminator on fakes\n",
    "        errorD_fake = criterion(output, label)\n",
    "        \n",
    "        #calculate the gradients for fakes and aggregate with gradients for the reals\n",
    "        errorD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        \n",
    "        #sum the errors\n",
    "        errorD = errorD_real + errorD_fake\n",
    "        \n",
    "        #finally, update discriminator\n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        #now basically do it all again for the generator this time\n",
    "        network_G.zero_grad()\n",
    "        label.fill_(real_label) #by convention fake labels are real for generator loss\n",
    "        \n",
    "        #since discriminator has updated, do another forward pass\n",
    "        #of all fakes through discriminator\n",
    "        output = network_D(fake).view(-1)\n",
    "        \n",
    "        #calculate generator loss based on updated output\n",
    "        errorG = criterion(output, label)\n",
    "        \n",
    "        #backprop to find gradients\n",
    "        errorG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        \n",
    "        #and finally, update generator\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        #not done yet though! we want to see and save progress, so\n",
    "        #we'll print out progress through every 50~500 iterations, since\n",
    "        #for each epoch we'll be iterating a *lot*.\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f' \n",
    "                 % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errorD.item(), errorG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        #record losses in lists created earlier\n",
    "        G_losses.append(errorG.item())\n",
    "        D_losses.append(errorD.item())\n",
    "        \n",
    "        #save some of the melspecs to img_list so we can actually convert them later\n",
    "        \n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with torch.no_grad():\n",
    "                fake = network_G(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding = 0, normalize = False))\n",
    "        \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once the training successfully finishes, print out training over time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
