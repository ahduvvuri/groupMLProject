{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm #progress bar!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from model/rnn.py\n",
    "\n",
    "class DelayedRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hp):\n",
    "    #should inherit all properties from all __init__ definitions\n",
    "        super(DelayedRNN, self).__init__()\n",
    "        self.num_hidden_layers = hp.model.hidden\n",
    "\n",
    "        self.t_delay_RNN_x = nn.LSTM(\n",
    "            input_size=self.num_hidden_layers,\n",
    "            hidden_size=self.num_hidden_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.t_delay_RNN_yz = nn.LSTM(\n",
    "            input_size=self.num_hidden_layers,\n",
    "            hidden_size=self.num_hidden_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # use central stack only at initial tier\n",
    "        self.c_RNN = nn.LSTM(\n",
    "            input_size=self.num_hidden_layers,\n",
    "            hidden_size=self.num_hidden_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.f_delay_RNN = nn.LSTM(\n",
    "            input_size=self.num_hidden_layers,\n",
    "            hidden_size=self.num_hidden_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.W_t = nn.Linear(3*self.num_hidden_layers, self.num_hidden_layers)\n",
    "        self.W_c = nn.Linear(self.num_hidden_layers, self.num_hidden_layers)\n",
    "        self.W_f = nn.Linear(self.num_hidden_layers, self.num_hidden_layers)\n",
    "        \n",
    "        def flatten_rnn(self):\n",
    "        self.t_delay_RNN_x.flatten_parameters()\n",
    "        self.t_delay_RNN_yz.flatten_parameters()\n",
    "        self.c_RNN.flatten_parameters()\n",
    "        self.f_delay_RNN.flatten_parameters()\n",
    "\n",
    "    def forward(self, input_h_t, input_h_f, input_h_c, audio_lengths):\n",
    "      \n",
    "        self.flatten_rnn()\n",
    "        # input_h_t, input_h_f: [B, M, T, D]\n",
    "        # input_h_c: [B, T, D]\n",
    "        B, M, T, D = input_h_t.size()\n",
    "\n",
    "        ####### time-delayed stack #######\n",
    "        # Fig. 2(a)-1 can be parallelized by viewing each horizontal line as batch\n",
    "        h_t_x_temp = input_h_t.view(-1, T, D)\n",
    "        h_t_x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            h_t_x_temp,\n",
    "            audio_lengths.unsqueeze(1).repeat(1, M).reshape(-1),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        h_t_x, _ = self.t_delay_RNN_x(h_t_x_packed)\n",
    "        h_t_x, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            h_t_x,\n",
    "            batch_first=True,\n",
    "            total_length=T\n",
    "        )\n",
    "        h_t_x = h_t_x.view(B, M, T, D)\n",
    "\n",
    "        # Fig. 2(a)-2,3 can be parallelized by viewing each vertical line as batch,\n",
    "        # using bi-directional version of GRU\n",
    "        h_t_yz_temp = input_h_t.transpose(1, 2).contiguous() # [B, T, M, D]\n",
    "        h_t_yz_temp = h_t_yz_temp.view(-1, M, D)\n",
    "        h_t_yz, _ = self.t_delay_RNN_yz(h_t_yz_temp)\n",
    "        h_t_yz = h_t_yz.view(B, T, M, 2*D)\n",
    "        h_t_yz = h_t_yz.transpose(1, 2)\n",
    "\n",
    "        h_t_concat = torch.cat((h_t_x, h_t_yz), dim=3)\n",
    "        output_h_t = input_h_t + self.W_t(h_t_concat) # residual connection, eq. (6)\n",
    "\n",
    "        ####### centralized stack #######\n",
    "        h_c_temp = nn.utils.rnn.pack_padded_sequence(\n",
    "            input_h_c,\n",
    "            audio_lengths,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        h_c_temp, _ = self.c_RNN(h_c_temp)\n",
    "        h_c_temp, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            h_c_temp,\n",
    "            batch_first=True,\n",
    "            total_length=T\n",
    "        )\n",
    "            \n",
    "        output_h_c = input_h_c + self.W_c(h_c_temp) # residual connection, eq. (11)\n",
    "        h_c_expanded = output_h_c.unsqueeze(1)\n",
    "\n",
    "        ####### frequency-delayed stack #######\n",
    "        h_f_sum = input_h_f + output_h_t + h_c_expanded\n",
    "        h_f_sum = h_f_sum.transpose(1, 2).contiguous() # [B, T, M, D]\n",
    "        h_f_sum = h_f_sum.view(-1, M, D)\n",
    "\n",
    "        h_f_temp, _ = self.f_delay_RNN(h_f_sum)\n",
    "        h_f_temp = h_f_temp.view(B, T, M, D)\n",
    "        h_f_temp = h_f_temp.transpose(1, 2) # [B, M, T, D]\n",
    "        \n",
    "        output_h_f = input_h_f + self.W_f(h_f_temp) # residual connection, eq. (8)\n",
    "\n",
    "        return output_h_t, output_h_f, output_h_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from model/tier.py\n",
    "\n",
    "class Tier(nn.Module):\n",
    "    \n",
    "    #args for __init__: \n",
    "    #hp = hyperparameters\n",
    "    #freq = frequency\n",
    "    #layers = number of total layers, hidden and otherwise\n",
    "    #tierN = what tier of computing we're at\n",
    "    \n",
    "    def __init__(self, hp, freq, layers, tierN): #initialize params\n",
    "        #should inherit all properties from all __init__ definitions\n",
    "        super(Tier, self).__init__()\n",
    "        num_hidden_layers = hp.model.hidden\n",
    "        self.hp = hp\n",
    "        self.tierN = tierN\n",
    "        \n",
    "        #initialize weights and run a different NN\n",
    "        #if we're just beginning or if we are continuing\n",
    "        if(tierN == 1): #if starting\n",
    "            #initialize weights to be ones for each hidden layer\n",
    "            self.W_t_0 = nn.Linear(1, num_hidden_layers)\n",
    "            self.W_f_0 = nn.Linear(1, num_hidden_layers)\n",
    "            self.W_c_0 = nn.Linear(freq, num_hidden_layers)\n",
    "            #initialize layers with DelayedRNN function\n",
    "            self.layers = nn.ModuleList([\n",
    "                DelayedRNN(hp) for _ in range(layers)\n",
    "            ])\n",
    "            \n",
    "        else: #if continuing\n",
    "            #reinitialize time weights\n",
    "            self.W_t = nn.Linear(1, num_hidden_layers)\n",
    "            #and upsample to higher resolution\n",
    "            self.layers = nn.ModuleList([\n",
    "                UpsampleRNN(hp) for _ in range(layers)\n",
    "            ])\n",
    "        \n",
    "        # Gaussian Mixture Model (GMM)\n",
    "        self.K = hp.model.gmm\n",
    "        self.pi_softmax = nn.Softmax(dim=3)\n",
    "        \n",
    "        # use correct mapping to produce GMM parameter\n",
    "        self.W_theta = nn.Linear(num_hidden_layers, 3*self.K)\n",
    "        \n",
    "    #args for forward:\n",
    "    #x = [B, M, T]; B = batch, M = mel, T = time\n",
    "    #audio_lengths = length of song\n",
    "    \n",
    "    def forward(self, x, audio_lengths):\n",
    "        #if beginning\n",
    "        if self.tierN == 1:\n",
    "            #make padding to ensure no out of bounds errors when running NN\n",
    "            #and unsqueeze the proper dims for each variable\n",
    "            h_t = self.W_t_0(F.pad(x, [1, -1]).unsqueeze(-1))\n",
    "            h_f = self.W_f_0(F.pad(x, [0, 0, 1, -1]).unsqueeze(-1))\n",
    "            h_c = self.W_c_0(F.pad(x, [1, -1]).transpose(1, 2))\n",
    "            for layer in self.layers:\n",
    "                h_t, h_f, h_c = layer(h_t, h_f, h_c, audio_lengths)\n",
    "\n",
    "            # h_t, h_f: [B, M, T, D] / D = num_hidden_layers\n",
    "            # h_c: [B, T, D]\n",
    "        #if continuing\n",
    "        else:\n",
    "            #update h_f from weights\n",
    "            h_f = self.W_t(x.unsqueeze(-1))\n",
    "            for layer in self.layers:\n",
    "                h_f = layer(h_f, audio_lengths)\n",
    "        \n",
    "        theta_hat = self.W_theta(h_f)\n",
    "\n",
    "        #formulae for updating mu, std, pi\n",
    "        mu = theta_hat[..., :self.K]\n",
    "        std = theta_hat[..., self.K:2*self.K]\n",
    "        pi = theta_hat[..., 2*self.K:]\n",
    "\n",
    "        return mu, std, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiers to divide time and freq axes depending on \n",
    "t_div = {1:1, 2:1, 3:2, 4:2, 5:4, 6:4}\n",
    "f_div = {1:1, 2:1, 3:2, 4:2, 5:4, 6:4, 7:8}\n",
    "\n",
    "class TierUtils():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining functions and classes for model to make things easier\n",
    "\n",
    "class MelNet_model(nn.Module): #from model.py from Deepest-Project\n",
    "    def __init__(self, hp, args, infer_hp):\n",
    "        #should inherit all properties from all __init__ definitions\n",
    "        super(MelNet, self).__init__()\n",
    "        self.hp = hp\n",
    "        self.args = args\n",
    "        self.infer_hp = infer_hp\n",
    "        self.f_div = f_div[hp.model.tier + 1]\n",
    "        self.t_div = t_div[hp.model.tier]\n",
    "        self.n_mels = hp.audio.n_mels\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
